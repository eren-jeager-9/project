{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Demo Emotion Recognition: audio + video"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Libraries and parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utilities\n",
    "import os\n",
    "import subprocess\n",
    "import numpy as np\n",
    "import keras\n",
    "\n",
    "# Audio and video manipulation\n",
    "import moviepy.editor as mp\n",
    "import cv2\n",
    "import librosa\n",
    "from joblib import load\n",
    "import tkinter as tk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Labels dictionary\n",
    "emotions_tras = {1:1, 2:4, 3:5, 4:0, 5:3, 6:2, 7:6}\n",
    "emotions = {0:'angry', 1:'calm', 2:'disgust', 3:'fear', 4:'happy', 5:'sad', 6:'surprise'}\n",
    "\n",
    "# Paths\n",
    "dataset_path = \"./Examples/\"\n",
    "haar_path = './../Other/haarcascade_frontalface_default.xml'\n",
    "parameters_path = './../Datasets/RAVDESS_audio/std_scaler.bin'\n",
    "models_video_path = \"./../Models/Video Stream/\"\n",
    "models_audio_path = \"./../Models/Audio Stream/\"\n",
    "vlc_path = \"C:/Program Files/VideoLAN/VLC/vlc.exe\" # to play the selected video (insert your own path to vlc.exe)\n",
    "\n",
    "# Audio video parameters\n",
    "height_targ = 112\n",
    "width_targ = 112\n",
    "sr = 48000"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Select Clip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "root= tk.Tk()\n",
    "\n",
    "canvas1 = tk.Canvas(root, width=400, height=300, relief='raised')\n",
    "canvas1.pack()\n",
    "\n",
    "label1 = tk.Label(root, text='Select clip to analize')\n",
    "label1.config(font=('helvetica', 16))\n",
    "canvas1.create_window(200, 25, window=label1)\n",
    "\n",
    "label2 = tk.Label(root, text='Number from 0 to 6:')\n",
    "label2.config(font=('helvetica', 11))\n",
    "canvas1.create_window(200, 100, window=label2)\n",
    "\n",
    "def display_text():\n",
    "   global example\n",
    "   example = int(example.get())\n",
    "   root.destroy\n",
    "\n",
    "example = tk.Entry(root)\n",
    "example.pack()\n",
    "canvas1.create_window(200, 140, window=example)\n",
    "\n",
    "    \n",
    "button1 = tk.Button(text='Select', command=lambda: [display_text(), root.destroy()], font=('helvetica', 12, 'bold'))\n",
    "canvas1.create_window(200, 180, window=button1)\n",
    "\n",
    "root.mainloop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "fn = os.listdir(dataset_path)\n",
    "filename = dataset_path + fn[example]\n",
    "label = emotions_tras[int(fn[example].split('-')[2]) - 1] # trasposition of the emotions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "player = subprocess.call([vlc_path, filename, '--play-and-exit'])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "3d4a2239",
   "metadata": {},
   "source": [
    "## Data preparation"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape frames: (32, 112, 112)\n"
     ]
    }
   ],
   "source": [
    "cap = cv2.VideoCapture(filename)\n",
    "haar_cascade = cv2.CascadeClassifier(haar_path)\n",
    "frames = []\n",
    "count = 0\n",
    "skip = 3\n",
    "\n",
    "# Loop through all frames\n",
    "while True:\n",
    "    # Capture frame\n",
    "    ret, frame = cap.read()\n",
    "    if (count % skip == 0 and count > 20):\n",
    "        if not ret:\n",
    "            break\n",
    "        frame = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "        # detect and crop face\n",
    "        faces = haar_cascade.detectMultiScale(frame, scaleFactor=1.12, minNeighbors=9)\n",
    "        if len(faces) != 1:\n",
    "            continue\n",
    "        for (x, y, w, h) in faces:\n",
    "            face = frame[y:y + h, x:x + w]\n",
    "\n",
    "        face = cv2.resize(face, (height_targ+10, width_targ+10))\n",
    "        face = face[5:-5, 5:-5]\n",
    "        face = face/255.\n",
    "        frames.append(face)\n",
    "    count += 1\n",
    "\n",
    "frames = np.array(frames)\n",
    "num_frames = len(frames)\n",
    "labels = [label] * num_frames\n",
    "print('shape frames:', frames.shape)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Audio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "audiofile = mp.AudioFileClip(filename).set_fps(sr)\n",
    "audio = audiofile.to_soundarray()\n",
    "audio = audio[int(sr/2):int(sr/2 + sr*3)]\n",
    "audio = np.array([elem[0] for elem in audio])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 128, 282, 1)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mel = librosa.power_to_db(librosa.feature.melspectrogram(audio, sr = 48000, n_fft = 1024, n_mels = 128, fmin = 50, fmax = 24000)) \n",
    "\n",
    "scaler = load(parameters_path)\n",
    "mel = scaler.transform(mel)\n",
    "\n",
    "mel = np.expand_dims(mel, axis = 2)\n",
    "mel = np.expand_dims(mel, axis = 0)\n",
    "mel.shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load models"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "models_list = os.listdir(models_video_path)\n",
    "\n",
    "acc = [float(model.split('[')[1].split(']')[0]) for model in models_list]\n",
    "idx = acc.index(max(acc))                                                       # index of best model\n",
    "\n",
    "model_video = keras.models.load_model(models_video_path + models_list[idx])\n",
    "# model_video.summary()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Audio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "models_list = os.listdir(models_audio_path)\n",
    "model_audio = keras.models.load_model(models_audio_path + models_list[0])\n",
    "# model_audio.summary()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predictions"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 2s 2s/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([3.90350557e-04, 2.60067428e-03, 5.59035325e-05, 1.19173186e-04,\n",
       "       9.39146042e-01, 6.43615067e-06, 5.76814897e-02], dtype=float32)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred = model_video.predict(frames)\n",
    "pred_video = np.mean(pred, axis=0)\n",
    "pred_video"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Audio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 275ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([2.4040800e-02, 5.5525964e-04, 7.1970987e-01, 1.8299127e-01,\n",
       "       3.4310304e-02, 3.6364559e-02, 2.0280003e-03], dtype=float32)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred = model_audio.predict(mel)\n",
    "pred_audio = np.mean(pred, axis=0)\n",
    "pred_audio"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Global"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_global = pred_video + pred_audio # mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Video prediction:\t happy\n",
      "Audio prediction:\t disgust\n",
      "Global prediction:\t happy\n",
      "Ground truth:\t\t happy\n"
     ]
    }
   ],
   "source": [
    "print('Video prediction:\\t', emotions[pred_video.argmax()])\n",
    "print('Audio prediction:\\t', emotions[pred_audio.argmax()])\n",
    "print('Global prediction:\\t', emotions[pred_global.argmax()])\n",
    "\n",
    "print('Ground truth:\\t\\t', emotions[label])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print Results\n",
    "root = tk.Tk()\n",
    "\n",
    "# root window title and dimension\n",
    "root.title(\"Results predictions\")\n",
    "\n",
    "canvas1 = tk.Canvas(root, width=400, height=300, relief='raised')\n",
    "canvas1.pack()\n",
    "\n",
    "label1 = tk.Label(root, text=f'Video prediction:\\t{emotions[pred_video.argmax()]}')\n",
    "label2 = tk.Label(root, text=f'Audio prediction:\\t{emotions[pred_audio.argmax()]}')\n",
    "label3 = tk.Label(root, text=f'Global prediction:\\t{emotions[pred_global.argmax()]}')\n",
    "label4 = tk.Label(root, text=f'Ground truth:\\t{emotions[label]}')\n",
    "\n",
    "label1.config(font=('helvetica', 14))\n",
    "label2.config(font=('helvetica', 14))\n",
    "label3.config(font=('helvetica', 14))\n",
    "label4.config(font=('helvetica', 14), fg='gray')\n",
    "\n",
    "canvas1.create_window(20, 25, window=label1, anchor='w')\n",
    "canvas1.create_window(20, 50, window=label2, anchor='w')\n",
    "canvas1.create_window(20, 75, window=label3, anchor='w')\n",
    "canvas1.create_window(20, 120, window=label4, anchor='w')\n",
    "\n",
    "button1 = tk.Button(text='Close', command=lambda: root.destroy(), font=('helvetica', 12, 'bold'))\n",
    "canvas1.create_window(200, 200, window=button1)\n",
    "\n",
    "root.mainloop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "ad2bdc8ecc057115af97d19610ffacc2b4e99fae6737bb82f5d7fb13d2f2c186"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
